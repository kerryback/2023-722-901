---
title:  "Forests"
author: <br><br><br><br><span style="font-family:perpetua; font-variant:small-caps; color:#606060;">Kerry Back</span><br><br><img src="RiceBusiness-transparent-logo-sm.png"  height=80>
execute:
  echo: false
  jupyter: python3
  cache: true
format: 
  revealjs:
    highlight-style: monokai
    code-fold: true
    code-copy: hover
    scrollable: true
    slide-number: true
    preview-links: true
    self-contained: true
    controls: true
    transition: fade
    theme: [solarized, 2023.scss]
    incremental: true
---

## Random Forests


- Create multiple random sets of training data
- Train a tree on each set and average the predictions
- To create a random set of training data,
  - Draw observations (rows) randomly from the original data with replacement
  - Draw as many observations as in the original data
- The random data is created and the trees are trained and averaged by scikit-learn's RandomForestRegressor or RandomForestClassifier

## Gradient Boosting

- Fit a tree to the training data
- Compute the errors from the tree, and fit a second tree to the errors
  - The predicted values are now the predictions from the first tree plus [a learning rate times]{style="color: tomato"} the predictions from the second tree
- Compute the errors and fit a third tree to the errors, etc.
- Learning rate < 1 avoids overshooting

## Adaptive Boosting

- Trees are fit sequentially
- Weights of observations are adjusted according to errors from current estimator
- Later trees focus on more difficult observations


```{python}
from sqlalchemy import create_engine
import pymssql
import pandas as pd

server = "mssql-82792-0.cloudclusters.net:16272"
username = "user"
password = "RiceOwls1912" # paste password between quote marks
database = "ghz"

string = "mssql+pymssql://" + username + ":" + password + "@" + server + "/" + database

conn = create_engine(string).connect()

df = pd.read_sql(
    """
    select ticker, date, agr, bm, idiovol, mom12m, roeq, ret
    from data
    where date in ('2021-12', '2022-01')
    """, 
    conn
)
features = ["agr", "bm", "idiovol", "mom12m", "roeq"]
df = df.dropna()
```



```{python}
from sklearn.preprocessing import QuantileTransformer
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import AdaBoostRegressor

qt = QuantileTransformer(output_distribution="normal")
def qtxs(d):
    x = qt.fit_transform(d)
    return pd.DataFrame(x, columns=d.columns, index=d.index)

df[features+["ret"]] = df.groupby(
  "date", 
  group_keys=False
)[features+["ret"]].apply(qtxs)

Xtrain = df[df.date=='2021-12'][features]
ytrain = df[df.date=='2021-12']["ret"]

Xtest = df[df.date=='2022-01'][features]
ytest = df[df.date=='2022-01']["ret"]
```

## Example

- Same data as in 3a-trees
  - agr, bm, idiovol, mom12m, roeq
  - training data = 2021-12
  - test data = 2022-01
- Quantile transform features and ret in each cross-section

# Random Forest

## Fitting a random forest
. . .

```{.p code-line-numbers="1|3-4|6-9|10"}
from sklearn.ensemble import RandomForestRegressor

Xtrain = df[df.date=='2021-12'][features]
ytrain = df[df.date=='2021-12']["ret"]

model = RandomForestRegressor(
  max_depth=3,
  random_state=0
)
model.fit(Xtrain, ytrain)
```

## Complexity and Scores

. . .

```{python}
import seaborn as sns
sns.set_style("whitegrid")
train_scores = []
test_scores = []
import numpy as np
grid = np.arange(1, 11)
for depth in grid:
  model = RandomForestRegressor(
  max_depth=depth,
  random_state=0
  )
  _=model.fit(Xtrain, ytrain)
  train_scores.append(model.score(Xtrain, ytrain))
  test_scores.append(model.score(Xtest, ytest))
```

```{python}
import matplotlib.pyplot as plt
plt.plot(grid, train_scores, label="train")
plt.plot(grid, test_scores, label="test")
plt.xlabel("Depth", fontsize=16)
plt.ylabel("Score", fontsize=16)
_ = plt.legend(fontsize=16)
```

# Gradient Boosting

## Fitting gradient boosting
. . .

```{.p code-line-numbers="1|3-7|8"}
from sklearn.ensemble import GradientBoostingRegressor

model = GradientBoostingRegressor(
  max_depth=3,
  learning_rate=0.05,
  random_state=0
)
model.fit(Xtrain, ytrain)
```


## Scores on Test Data

. . .

```{python}
import seaborn as sns
sns.set_style("whitegrid")
import numpy as np
grid = np.arange(1, 11)
scores=pd.DataFrame(columns=[0.01, 0.05, 0.1, 0.2], index=grid)


for depth in grid:
  for lr in scores.columns:
      model = GradientBoostingRegressor(
      max_depth=depth,
      learning_rate=lr,
      random_state=0
      )
      _=model.fit(Xtrain, ytrain)
      scores.loc[depth, lr] = model.score(Xtest, ytest)
```

```{python}
import matplotlib.pyplot as plt
for lr in scores.columns:
  scores[lr].plot(label=f"learning rate = {lr}")
plt.xlabel('Depth', fontsize=16)
plt.ylabel('Score', fontsize=16)
_ = plt.legend(fontsize=16)
```

# Adaptive Boosting

## Fitting adaptive boosting
. . .

```{.p code-line-numbers="1-2|4-10|11"}
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor

model = AdaBoostRegressor(
    DecisionTreeRegressor(
        max_depth=3,
        random_state=0
    ),
    learning_rate=0.5,
)
model.fit(Xtrain, ytrain)
```


## Scores on Test Data

. . .

```{python}
from sklearn.tree import DecisionTreeRegressor

grid = np.arange(1, 11)
scores=pd.DataFrame(columns=[0.05, 0.1, 0.2, 0.5], index=grid)


for depth in grid:
  for lr in scores.columns:
    model = AdaBoostRegressor(
      DecisionTreeRegressor(
        max_depth=depth,
        random_state=0
      ),
    learning_rate=lr,
    )
    _=model.fit(Xtrain, ytrain)
    scores.loc[depth, lr] = model.score(Xtest, ytest)
```

```{python}
import matplotlib.pyplot as plt
for lr in scores.columns:
  scores[lr].plot(label=f"learning rate = {lr}")
plt.xlabel('Depth', fontsize=16)
plt.ylabel('Score', fontsize=16)
_ = plt.legend(fontsize=16)
```