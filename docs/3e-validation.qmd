---
title:  "Cross Validation"
author: <br><br><br><br><span style="font-family:perpetua; font-variant:small-caps; color:#606060;">Kerry Back</span><br><br><img src="RiceBusiness-transparent-logo-sm.png"  height=80>
execute:
  echo: false
  jupyter: python3
  cache: true
format: 
  revealjs:
    highlight-style: monokai
    code-fold: true
    code-copy: hover
    scrollable: true
    slide-number: true
    preview-links: true
    self-contained: true
    controls: true
    transition: fade
    theme: [solarized, 2023.scss]
    incremental: true
---

##

- Cross-validation (CV) is a way to choose optimal hyperparameters using the training data
- Split the training data into subsets, e.g., A, B, C, D, E
- Define a finite set of hyperparemeter combinations to choose from (the grid)
  - Example: depth, learning rate = [[4, 0.1], [4, 0.5], [6, 0.1], [6, 0.5]]
  - Example: hidden layer sizes = [[4, 2], [8, 4, 2], [16, 8, 4]]]

## 

- Use one of the subsets (e.g., A) as the validation set
- Train with each of the hyperparameter combinations on the union of the remaining subsets (e.g., B $\cup$ C $\cup$ D $\cup$ E)
- Compute the trained model scores on A
- Repeat with B as the validation set, etc.
- For each hyperparameter combination, end up with as many validation scores as there are subsets

##

- Average the validation scores to get a single score for each hyperparameter combination
- Choose the hyperparameters with the highest average score
- All of this together is "search over the grid using cross-validation to find the best hyperparameters"
- It is implemented by scikit-learn's GridSearchCV function

## Custom scoring function

- If we want to use predictions to sort stocks into "good" and "bad", then MSE is a poor scoring method.
- Example: if you buy a stock because the predicted return is 40% and the actual return turns out to be 100%, is that really an error?
- We want higher predictions to correspond to higher actual returns, so rank correlation would be a reasonable scoring method.

## Coding a custom scoring function

. . .

```.p
from scipy.stats import spearmanr
from sklearn.metrics import make_scorer
score = make_scorer(spearmanr, greater_is_better=True)
```

- We can pass this to GridSearchCV to choose the best hyperparameters.

## Example

- Same data as in 3a-trees
  - agr, bm, idiovol, mom12m, roeq
  - training data = 2021-12
  - test data = 2022-01
- Quantile transform features and ret in each cross-section



```{python}
from sqlalchemy import create_engine
import pymssql
import pandas as pd

server = "mssql-82792-0.cloudclusters.net:16272"
username = "user"
password = "RiceOwls1912" # paste password between quote marks
database = "ghz"

string = "mssql+pymssql://" + username + ":" + password + "@" + server + "/" + database

conn = create_engine(string).connect()

df = pd.read_sql(
    """
    select ticker, date, agr, bm, idiovol, mom12m, roeq, ret
    from data
    where date in ('2021-12', '2022-01')
    """, 
    conn
)
features = ["agr", "bm", "idiovol", "mom12m", "roeq"]
df = df.dropna()
```



```{python}
from sklearn.preprocessing import QuantileTransformer
qt = QuantileTransformer(output_distribution="normal")
def qtxs(d):
    x = qt.fit_transform(d)
    return pd.DataFrame(x, columns=d.columns, index=d.index)

df[features+["ret"]] = df.groupby(
  "date", 
  group_keys=False
)[[features+["ret"]].apply(qtxs)
```

## Cross validate AdaBoost

. . .

```{.p code-line-numbers="1|3-4|6-9|10"}
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor

model = AdaBoostRegressor(
    DecisionTreeRegressor(
        max_depth=3,
        random_state=0
    ),
    learning_rate=0.5,
)

Xtrain = df[df.date=='2021-12'][features]
ytrain = df[df.date=='2021-12']["ret"]

model = DecisionTreeRegressor(
  max_depth=2,
  random_state=0
)
model.fit(Xtrain, ytrain)
```

```{python}
from sklearn.tree import DecisionTreeRegressor

Xtrain = df[df.date=='2021-12'][["roeq", "mom12m"]]
ytrain = df[df.date=='2021-12']["ret"]

model = DecisionTreeRegressor(
  max_depth=2,
  random_state=0
)
_=model.fit(Xtrain, ytrain)
```

## View the regression tree

. . .

```p
from sklearn.tree import plot_tree
_ = plot_tree(model)
```

. . .

```{python}
import matplotlib as mpl
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

mpl.rcParams["figure.figsize"] = (16,5)

_ = plot_tree(model)
```

## Feature importance

- What fraction of the splitting is each feature responsible for?

. . .

```p
pd.Series(model.feature_importances_, index=Xtrain.columns)
```

. . .

```{python}
pd.Series(model.feature_importances_, index=Xtrain.columns)
```

## Model score

- The default scoring method for regression models is R-squared.
- Equals 1 - MSE / mean squared deviation from mean
  - MSE = mean squared error
  - So we compare MSE from the model to MSE using the mean to predict
- Max R-squared is 1 and can be negative

##

- R-squared on training data

. . .

```.p
model.score(Xtrain, ytrain)
```
. . .

```{python}
model.score(Xtrain, ytrain)
```

- R-squared on test data

. . .

```.p
Xtest = df[df.date=='2022-01'][["roeq", "mom12m"]]
ytest = df[df.date=='2022-01']["ret"]
model.score(Xtest, ytest)
```

. . .

```{python}
Xtest = df[df.date=='2022-01'][["roeq", "mom12m"]]
ytest = df[df.date=='2022-01']["ret"]
model.score(Xtest, ytest)
```

## Depth and overfitting

- If we make a model more complex (more parameters), it will fit the training data better but may make worse predictions on new data.
- This is called overfitting.
- We can make a tree more complex by increasing its depth.

## Overfitting our tree

. . .

```{python}
import seaborn as sns
sns.set_style("whitegrid")
train_scores = []
test_scores = []
import numpy as np
grid = np.arange(2, 22, 2)
for depth in grid:
  model = DecisionTreeRegressor(
  max_depth=depth,
  random_state=0
  )
  _=model.fit(Xtrain, ytrain)
  train_scores.append(model.score(Xtrain, ytrain))
  test_scores.append(model.score(Xtest, ytest))
```

```{python}
plt.plot(grid, train_scores, label="train")
plt.plot(grid, test_scores, label="test")
plt.xlabel("Depth", fontsize=16)
plt.ylabel("Score", fontsize=16)
_ = plt.legend(fontsize=16)
```