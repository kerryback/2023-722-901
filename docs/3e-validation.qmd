---
title:  "Cross Validation"
author: <br><br><br><br><span style="font-family:perpetua; font-variant:small-caps; color:#606060;">Kerry Back</span><br><br><img src="RiceBusiness-transparent-logo-sm.png"  height=80>
execute:
  echo: false
  jupyter: python3
  cache: true
format: 
  revealjs:
    highlight-style: monokai
    code-fold: true
    code-copy: hover
    scrollable: true
    slide-number: true
    preview-links: true
    self-contained: true
    controls: true
    transition: fade
    theme: [solarized, 2023.scss]
    incremental: true
---

##

- Cross-validation (CV) is a way to choose optimal hyperparameters using the training data
- Split the training data into subsets, e.g., A, B, C, D, E
- Define a finite set of hyperparemeter combinations (a grid) to choose from
  - Example: {"max_depth": [3, 4], "learning_rate": [0.05, 0.1]}
  - Example: {"hidden_layer_sizes:[[4, 2], [8, 4, 2], [16, 8, 4]]}

## 

- Use one of the subsets (e.g., A) as the validation set
- Train with each of the hyperparameter combinations on the union of the remaining subsets (e.g., B $\cup$ C $\cup$ D $\cup$ E)
- Compute the trained model scores on A
- Repeat with B as the validation set, etc.
- For each hyperparameter combination, end up with as many validation scores as there are subsets

##

- Average the validation scores to get a single score for each hyperparameter combination
- Choose the hyperparameters with the highest average score
- All of this together is "search over the grid using cross-validation to find the best hyperparameters"
- It is implemented by scikit-learn's GridSearchCV function

## Example

- Same data as in 3a-trees
  - agr, bm, idiovol, mom12m, roeq
  - data = 2021-12 (training data)
- Quantile transform features and ret 



```{python}
from sqlalchemy import create_engine
import pymssql
import pandas as pd

server = "mssql-82792-0.cloudclusters.net:16272"
username = "user"
password = "RiceOwls1912" # paste password between quote marks
database = "ghz"

string = "mssql+pymssql://" + username + ":" + password + "@" + server + "/" + database

conn = create_engine(string).connect()

df = pd.read_sql(
    """
    select ticker, date, agr, bm, idiovol, mom12m, roeq, ret
    from data
    where date = '2021-12'
    """, 
    conn
)
features = ["agr", "bm", "idiovol", "mom12m", "roeq"]
df = df.dropna()
```



```{python}
from sklearn.preprocessing import QuantileTransformer
qt = QuantileTransformer(output_distribution="normal")
def qtxs(d):
    x = qt.fit_transform(d)
    return pd.DataFrame(x, columns=d.columns, index=d.index)

df[features+["ret"]] = df.groupby(
  "date", 
  group_keys=False
)[features+["ret"]].apply(qtxs)
```

## Cross validate gradient boosting

. . .

```.p
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import GridSearchCV
```

##


```{.p code-line-numbers="1-4|6-9|11|12"}
param_grid = {
  "max_depth": [3, 4], 
  "learning_rate": [0.05, 0.1]
}

cv = GridSearchCV(
  estimator=GradientBoostingRegressor(),
  param_grid=param_grid,
)

_ = cv.fit(Xtrain, ytrain)
pd.DataFrame(cv.cv_results_).iloc[:, 4:]
```

## {.smaller}

```{python}
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingRegressor

model = GradientBoostingRegressor()

Xtrain = df[features]
ytrain = df["ret"]

param_grid = {
  "max_depth": [3, 4], 
  "learning_rate": [0.05, 0.1]
}

cv = GridSearchCV(
  model,
  param_grid=param_grid,
)

_ = cv.fit(Xtrain, ytrain)
pd.DataFrame(cv.cv_results_).iloc[:, 4:]
```
